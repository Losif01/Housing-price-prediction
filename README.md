# Housing price prediction

## Overview 
This Data science project analyses the [Housing Price Dataset]([House Prices - Advanced Regression Techniques | Kaggle](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques)), predicting the `SalePrice` based on all other features in the dataset
**Technologies used:**
- `python 3.12`
- `matplotlib`
- `pandas`
- `numpy`
- `seaborn`
- `XGBRegressor`
- `Sklearn`
## Step 1: Exploratory Data Analysis 

- Know the null and unique values in each column in the dataset 
- Differentiating between object and numeric values for their corresponding suitable data cleaning technique 
- Plotted the numeric data correlation with the target column `SalePrice`
- Plotted a frequency histogram for each numeric column in the dataset
![[Pasted image 20250211205718.png]]

- Created a Correlation `Heatmap` 
![[Pasted image 20250211205635.png]]

- Created a function to find all null values in a column, which was useful in the long run
```python
def getMissingValues(df):
Â  Â  missing_values = df.isnull().sum().sort_values(ascending=False)
Â  Â  missing_values = missing_values[missing_values > 0]
Â  Â  missing_values = missing_values / len(df)
Â  Â  return [missing_values], missing_values.__len__()
```

- Created a function to find all unique values in the dataset
```python
def getUniqueVaules(df):

Â  Â  unique_values = []
Â  Â  for i in df.columns:
Â  Â  Â  Â  unique_values.append((i , df[i].nunique()))
Â  Â  return f" {unique_values} "
```

- Plotted a frequency histogram for each categorical column, and filled the null values with suitable 
- Handled numeric null records with `Measure of Center` techniques (mean, median, mode)
- Export cleaned data to new files

--- 
## Step 2: Feature Engineering
- Added the following columns that are derived from other columns:
```python 
train['TotalArea'] = train['TotalBsmtSF'] + train['1stFlrSF'] + train['2ndFlrSF']

test['TotalArea'] = test['TotalBsmtSF'] + test['1stFlrSF'] + test['2ndFlrSF']

# Age of the house when sold
train['HouseAge'] = train['YrSold'] - train['YearBuilt']

test['HouseAge'] = test['YrSold'] - test['YearBuilt']

# Interaction term: Overall quality * Living area
train['QualLivArea'] = train['OverallQual'] * train['GrLivArea']

test['QualLivArea'] = test['OverallQual'] * test['GrLivArea']
```

- used the `log transform` to convert skewed data into an acceptable domain for training
```python
skewed_features = ['GrLivArea', 'TotalBsmtSF', '1stFlrSF']

  

for col in skewed_features:
Â  Â  train[col] = np.log1p(train[col])
Â  Â  test[col] = np.log1p(test[col])

  

# Log-transform the target variable
y_train = np.log1p(y_train) Â # Reverse later with np.expm1()
```

- used `Ordinal encoding` for some columns 
```python 
# Define ordinal mappings (customize based on your data)

ordinal_mappings = {

Â  Â  'ExterQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1},

Â  Â  'BsmtExposure': {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'None': 0},

Â  Â  'KitchenQual': {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1}

}

  

for col, mapping in ordinal_mappings.items():

Â  Â  train[col] = train[col].map(mapping)

Â  Â  test[col] = test[col].map(mapping).fillna(0) Â # Handle missing/unseen categories in test
```

- Saved the processed data for the next Stage: *Model building*

---

## **Step 3: Model Building _`XGBregressor`_  

## **Introduction**  
`XGBRegressor` is a regression model from the XGBoost (Extreme Gradient Boosting) library, optimized for speed and performance. It builds an ensemble of decision trees iteratively to minimize a loss function.  

## **Core Concept**  
XGBoost is a boosting algorithm that builds models additively by minimizing a differentiable loss function using gradient descent.  

### **Mathematical Formulation**  
Given a dataset `{(x_i, y_i)}_{i=1}^{n}`, where:  
- `x_i` is the feature vector for the `i`th data point,  
- `y_i` is the target value.  

We aim to predict `Å·_i` using an ensemble of `K` regression trees:  

`Å·_i = Î£_{k=1}^{K} f_k(x_i)`,  

where `f_k(x)` represents the `k`th regression tree.  

### **Objective Function**  
XGBoost optimizes the following objective function:  

`L(Î˜) = Î£_{i=1}^{n} l(y_i, Å·_i) + Î£_{k=1}^{K} Î©(f_k)`,  

where:  
- `l(y_i, Å·_i)` is a differentiable convex loss function (e.g., Mean Squared Error).  
- `Î©(f_k)` is a regularization term to prevent overfitting.  

#### **Loss Function (Squared Error for Regression)**  
For regression, the most common loss function is the squared error:  

`l(y_i, Å·_i) = (y_i - Å·_i)^2`,  

which leads to gradient boosting minimizing:  

`Î£_{i=1}^{n} (y_i - Å·_i)^2 + Î£_{k=1}^{K} Î©(f_k)`.  

#### **Regularization Term**  
To control model complexity, XGBoost includes a regularization term:  

`Î©(f) = Î³T + (1/2) Î» Î£_{j} w_j^2`,  

where:  
- `T` is the number of leaf nodes in the tree.  
- `w_j` is the weight of leaf `j`.  
- `Î³` and `Î»` are regularization parameters.  

### **Tree Growth & Optimization**  
At each iteration, a new tree is added to the model to minimize residuals. The weights of the tree are computed using the second-order Taylor expansion:  

`g_i = âˆ‚ l(y_i, Å·_i) / âˆ‚ Å·_i,`  
`h_i = âˆ‚Â² l(y_i, Å·_i) / âˆ‚ Å·_iÂ²,`  

where:  
- `g_i` is the gradient (first derivative of loss).  
- `h_i` is the Hessian (second derivative of loss).  

For each leaf `j`, the optimal weight `w_j` is given by:  

`w_j* = - (Î£_{i âˆˆ I_j} g_i) / (Î£_{i âˆˆ I_j} h_i + Î»)`,  

where `I_j` represents the set of samples in leaf `j`.  

### **Final Prediction**  
The final prediction is computed as:  

`Å·_i = F_K(x_i) = F_{K-1}(x_i) + f_K(x_i)`,  

where `F_K(x)` is the cumulative model up to the `K`th tree.  

## **Hyperparameters of `XGBRegressor`**  
Key hyperparameters include:  
- `n_estimators`: Number of trees.  
- `learning_rate` (`Î·`): Step size shrinkage.  
- `max_depth`: Maximum depth of trees.  
- `lambda`: L2 regularization term.  
- `gamma`: Minimum loss reduction for a split.  
- `subsample`: Fraction of samples used per tree.  
- `colsample_bytree`: Fraction of features used per tree.  

## **Conclusion**  
`XGBRegressor` is a powerful and efficient gradient boosting algorithm designed for regression tasks. It minimizes a loss function using additive tree models while incorporating regularization for better generalization.  

---

This version should now be fully compatible with GitHub Markdown preview. Let me know if you need any tweaks! ðŸš€

## **Conclusion**  
`XGBRegressor` is a powerful and efficient gradient boosting algorithm designed for regression tasks. It minimizes a loss function using additive tree models while incorporating regularization for better generalization.  

---

